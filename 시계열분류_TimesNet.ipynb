{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "JtRjs3YEITvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def FFT_for_Period(x, k=2):\n",
        "    xf = torch.fft.rfft(x, dim=1)\n",
        "    frequency_list = abs(xf).mean(0).mean(-1)\n",
        "    frequency_list[0] = 0\n",
        "\n",
        "    _, top_list = torch.topk(frequency_list, k)\n",
        "    top_list = top_list.detach().cpu().numpy()\n",
        "    period = x.shape[1] // top_list\n",
        "    return period, abs(xf).mean(-1)[:, top_list]\n",
        "\n",
        "class Inception_Block_V1(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):\n",
        "        super(Inception_Block_V1, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.num_kernels = num_kernels\n",
        "        kernels = []\n",
        "\n",
        "        for i in range(self.num_kernels):\n",
        "            kernels.append(nn.Conv3d(in_channels, out_channels, kernel_size=(3, 3, 3), padding=(1, 1, 1)))\n",
        "\n",
        "        self.kernels = nn.ModuleList(kernels)\n",
        "\n",
        "        if init_weight:\n",
        "            self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv3d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        res_list = []\n",
        "        for i in range(self.num_kernels):\n",
        "            res_list.append(self.kernels[i](x))\n",
        "        res = torch.stack(res_list, dim=-1).mean(-1)\n",
        "        return res\n",
        "\n",
        "class TimesBlock(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(TimesBlock, self).__init__()\n",
        "        self.seq_len = configs.seq_len\n",
        "        self.k = configs.top_k\n",
        "        self.conv = nn.Sequential(\n",
        "            Inception_Block_V1(configs.d_model, configs.d_ff, num_kernels=configs.num_kernels),\n",
        "            nn.GELU(),\n",
        "            Inception_Block_V1(configs.d_ff, configs.d_model, num_kernels=configs.num_kernels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, N = x.size()\n",
        "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
        "        res = []\n",
        "        for i in range(self.k):\n",
        "            period = period_list[i]\n",
        "            if self.seq_len % period != 0:\n",
        "                length = ((self.seq_len // period) + 1) * period\n",
        "                padding = torch.zeros([x.shape[0], (length - self.seq_len), x.shape[2]]).to(x.device)\n",
        "                out = torch.cat([x, padding], dim=1)\n",
        "            else:\n",
        "                length = self.seq_len\n",
        "                out = x\n",
        "\n",
        "            out = out.reshape(B, length // period, period, N).permute(0, 3, 1, 2).contiguous()\n",
        "            out = self.conv(out)\n",
        "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
        "            res.append(out[:, :self.seq_len, :])\n",
        "\n",
        "        res = torch.stack(res, dim=-1)\n",
        "        period_weight = F.softmax(period_weight, dim=1)\n",
        "        period_weight = period_weight.unsqueeze(1).unsqueeze(1).repeat(1, T, N, 1)\n",
        "        res = torch.sum(res * period_weight, -1)\n",
        "        res = res + x\n",
        "        return res\n",
        "\n",
        "class JointEmbedding(nn.Module):\n",
        "    def __init__(self, d_model):\n",
        "        super(JointEmbedding, self).__init__()\n",
        "        self.embedding = nn.Linear(2, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, J = x.shape\n",
        "        num_joints = J // 2\n",
        "        x = x.view(B, T, num_joints, 2)\n",
        "        x = self.embedding(x)\n",
        "        x = x.mean(dim=2)\n",
        "        return x\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model).float()\n",
        "        pe.require_grad = False\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pe[:, :x.size(1)]\n",
        "\n",
        "class DataEmbedding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1):\n",
        "        super(DataEmbedding, self).__init__()\n",
        "        self.value_embedding = JointEmbedding(d_model=d_model)\n",
        "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.value_embedding(x) + self.position_embedding(x)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, configs):\n",
        "        super(Model, self).__init__()\n",
        "        self.configs = configs\n",
        "        self.seq_len = configs.seq_len\n",
        "        self.model = nn.ModuleList([TimesBlock(configs) for _ in range(configs.e_layers)])\n",
        "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
        "        self.layer = configs.e_layers\n",
        "        self.layer_norm = nn.LayerNorm(configs.d_model)\n",
        "\n",
        "        self.act = F.gelu\n",
        "        self.dropout = nn.Dropout(configs.dropout)\n",
        "        self.projection = nn.Linear(configs.d_model * configs.seq_len, configs.num_class)\n",
        "\n",
        "    def classification(self, x_enc, x_mark_enc):\n",
        "        enc_out = self.enc_embedding(x_enc, None)\n",
        "\n",
        "        for i in range(self.layer):\n",
        "            enc_out = self.layer_norm(self.model[i](enc_out))\n",
        "\n",
        "        output = self.act(enc_out)\n",
        "        output = self.dropout(output)\n",
        "        output = output * x_mark_enc.unsqueeze(-1)\n",
        "        output = output.reshape(output.shape[0], -1)\n",
        "        output = self.projection(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, x_enc, x_mark_enc):\n",
        "        output = self.classification(x_enc, x_mark_enc)\n",
        "        return output"
      ],
      "metadata": {
        "id": "Oxrx6Z2lIQsq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
